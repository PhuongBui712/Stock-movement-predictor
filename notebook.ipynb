{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from typing import Tuple, List, Optional, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "# read\n",
    "for name in os.listdir(\"./data/\"):\n",
    "    df = pd.read_csv(os.path.join(\"./data\", name))\n",
    "    df[\"Date/Time\"] = pd.to_datetime(df[\"Date/Time\"], format=\"%m/%d/%Y %H:%M\")\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "    print(name)\n",
    "    display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's nothing noticeable excepts the values of `Open Interest` column are always 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Drop `Open Interest` column**\n",
    "\n",
    "As we described above, the `Open Interest` doesn't take any useful data for training, so we will drop it for saving resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dfs)):\n",
    "    dfs[i] = dfs[i].drop(\"Open Interest\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Missing timestamp**\n",
    "\n",
    "There are some missing timestamps in the input data. The dataset should contain all stock data in each minute between 9:00 to 11:29 and 13:00 to 14:46 from Monday to Friday, but it is often missed such as following example:\n",
    "\n",
    "```csv\n",
    "Ticker,Date/Time,Open,High,Low,Close,Volume,Open Interest\n",
    "FPT,12/25/2018 9:15,30.89,30.89,30.89,30.89,35410,0\n",
    "FPT,12/25/2018 9:16,30.81,30.81,30.81,30.81,190,0\n",
    "FPT,12/25/2018 9:17,30.74,30.81,30.74,30.74,1120,0\n",
    "FPT,12/25/2018 9:18,30.74,30.74,30.74,30.74,2120,0\n",
    "FPT,12/25/2018 9:19,30.74,30.74,30.74,30.74,22500,0\n",
    "FPT,12/25/2018 9:20,30.74,30.74,30.7,30.74,7140,0\n",
    "FPT,12/25/2018 9:21,30.66,30.74,30.59,30.66,16480,0\n",
    "```\n",
    "\n",
    "Therefore, whatever the reason for data loss, we assume that stock market always opens during the above mentioned time periods. We will fill missing timestamp in existing day (a day which have availably had trading data) by forward filling method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minute_range(date):\n",
    "    morning_range = pd.date_range(\n",
    "        start=f\"{date} 09:15\", end=f\"{date} 11:29\", freq=\"1min\"\n",
    "    )\n",
    "    afternoon_range = pd.date_range(\n",
    "        start=f\"{date} 13:00\", end=f\"{date} 14:46\", freq=\"1min\"\n",
    "    )\n",
    "    return morning_range.union(afternoon_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filling_missing_timestamp(df: DataFrame) -> DataFrame:\n",
    "    # Set 'Date/Time' as the index\n",
    "    df.set_index(\"Date/Time\", inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # Get unique dates in the dataset\n",
    "    unique_dates = pd.Series(df.index.date).unique()\n",
    "\n",
    "    all_minutes = []\n",
    "    for date in unique_dates:\n",
    "        all_minutes.extend(create_minute_range(date))\n",
    "\n",
    "    # Reindex the dataframe with the new index\n",
    "    new_index = pd.DatetimeIndex(all_minutes)\n",
    "    df_filled = df.reindex(new_index)\n",
    "\n",
    "    # Forward fill the missing values\n",
    "    df_filled = df_filled.ffill()\n",
    "    if df_filled.isna().any().any():\n",
    "        df_filled = df_filled.bfill()\n",
    "\n",
    "    # Reset the index to make 'Date/Time' a column again\n",
    "    df_filled.reset_index(inplace=True)\n",
    "    df_filled.rename(columns={\"index\": \"Date/Time\"}, inplace=True)\n",
    "\n",
    "    return df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling\n",
    "for i in range(len(dfs)):\n",
    "    dfs[i] = filling_missing_timestamp(dfs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. One - hot encoding `Ticker` column**\n",
    "\n",
    "Because of our goal is developing a general model to predict stock price movement for all stocks, we have to transform column `Ticker` to numerical for training model later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode_dataframe(\n",
    "    df: DataFrame, column_name: Optional[str] = None, categories: Optional[List] = None\n",
    ") -> DataFrame:\n",
    "    if (column_name and categories is None) or (column_name is None and categories):\n",
    "        raise ValueError(\n",
    "            \"Either both 'column_name' and 'categories' must be provided, or neither should be provided.\"\n",
    "        )\n",
    "\n",
    "    dummy_df = df.copy()\n",
    "    if column_name:\n",
    "        column_df = pd.DataFrame({column_name: categories})\n",
    "        dummy_df = pd.concat([dummy_df, column_df], axis=0, ignore_index=True)\n",
    "\n",
    "    dummy_df = pd.get_dummies(dummy_df)\n",
    "\n",
    "    if column_name:\n",
    "        return dummy_df.iloc[: -len(categories)]\n",
    "\n",
    "    return dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = [\"FPT\", \"MSN\", \"PNJ\", \"VIC\"]\n",
    "for i in range(len(dfs)):\n",
    "    dfs[i] = onehot_encode_dataframe(\n",
    "        dfs[i], column_name=\"Ticker\", categories=ticker_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Other stuffs**\n",
    "\n",
    "Finally, we have to do some small preprocessing for further processing such as drop `Date/Time` columns and astype to `float`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dfs)):\n",
    "    # drop `Date/Time`\n",
    "    dfs[i] = dfs[i].drop(\"Date/Time\", axis=1)\n",
    "\n",
    "    # astype\n",
    "    dfs[i] = dfs[i].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicing_window(\n",
    "    df: DataFrame,\n",
    "    label_name: Union[str, List[str]],\n",
    "    start_idx: int = 0,\n",
    "    input_size: int = 30,\n",
    "    offset: int = 1,\n",
    "    end_idx: Optional[int] = None,\n",
    "    label_size: Optional[int] = None,\n",
    ") -> Tuple[List, List]:\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    start_idx += input_size + offset\n",
    "    if end_idx:\n",
    "        end_idx = len(df) - label_size - offset\n",
    "\n",
    "    for idx in range(start_idx, end_idx):\n",
    "        feature_start_idx = idx - input_size - offset\n",
    "        feature_end_idx = idx - offset\n",
    "\n",
    "        feature = df.loc[feature_start_idx:feature_end_idx, :]\n",
    "        label = df.loc[feature_start_idx:feature_end_idx, label_name]\n",
    "\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_trains = []\n",
    "# y_trains = []\n",
    "# X_vals = []\n",
    "# y_vals = []\n",
    "# X_tests = []\n",
    "# y_tests = []\n",
    "\n",
    "# for i in range(len(dfs)):\n",
    "#     train_end_idx = int(TRAIN_SIZE * len(dfs[i]))\n",
    "#     val_end_idx = int(VAL_SIZE * len(dfs[i])) + train_end_idx\n",
    "\n",
    "#     X_train, y_train = slicing_window(\n",
    "#         dfs[i],\n",
    "#         label_name=LABEL_NAMES,\n",
    "#         start_idx=0,\n",
    "#         input_size=INPUT_SIZE,\n",
    "#         end_idx=train_end_idx,\n",
    "#         label_size=TIMESTAMP_PER_DAY, # predict to next day\n",
    "#     )\n",
    "\n",
    "#     # X_val, y_val = slicing_window(\n",
    "#     #     dfs[i],\n",
    "#     #     label_name=LABEL_NAMES,\n",
    "#     #     start_idx=train_end_idx,\n",
    "#     #     input_size=INPUT_SIZE,\n",
    "#     #     end_idx=val_end_idx,\n",
    "#     #     label_size=TIMESTAMP_PER_DAY\n",
    "#     # )\n",
    "\n",
    "#     # X_test, y_test = slicing_window(\n",
    "#     #     dfs[i],\n",
    "#     #     label_name=LABEL_NAMES,\n",
    "#     #     start_idx=val_end_idx,\n",
    "#     #     input_size=INPUT_SIZE,\n",
    "#     #     end_idx=len(dfs[i]),\n",
    "#     #     label_size=TIMESTAMP_PER_DAY\n",
    "#     # )\n",
    "\n",
    "#     X_trains.append(X_train)\n",
    "#     # X_vals.append(X_val)\n",
    "#     # X_test.append(X_test)\n",
    "#     y_trains.append(y_train)\n",
    "#     # y_vals.append(y_val)\n",
    "#     # y_tests.append(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        label_name: Union[str, List[str]],\n",
    "        start_idx: int = 0,\n",
    "        input_size: int = 30,\n",
    "        offset: int = 1,\n",
    "        end_idx: Optional[int] = None,\n",
    "        label_size: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.start_idx = start_idx\n",
    "        self.input_size = input_size\n",
    "        self.offset = offset\n",
    "        self.end_idx = end_idx\n",
    "        self.label_size = label_size\n",
    "        self.label_name = label_name\n",
    "        if isinstance(self.label_name, str):\n",
    "            self.label_name = [self.label_name]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.end_idx - self.label_size - self.offset - self.start_idx\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_x = self.start_idx + idx\n",
    "        end_x = start_x + self.input_size\n",
    "        start_y = end_x + self.offset\n",
    "        end_y = start_y + self.label_size\n",
    "\n",
    "        features = torch.tensor(self.df.iloc[start_x:end_x].values, dtype=torch.float32)\n",
    "        labels = torch.tensor(self.df.loc[start_y:end_y-1, self.label_name].values, dtype=torch.float32)\n",
    "\n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP_PER_DAY = 242\n",
    "INPUT_SIZE = 3 * TIMESTAMP_PER_DAY  # 3 days\n",
    "OFFSET = 1\n",
    "LABEL_NAMES = dfs[0].columns.tolist()\n",
    "\n",
    "TRAIN_SIZE = 0.7\n",
    "VAL_SIZE = 0.2\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_list = []\n",
    "val_dataset_list = []\n",
    "test_dataset_list = []\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    train_end_idx = int(TRAIN_SIZE * len(dfs[i]))\n",
    "    val_end_idx = int(VAL_SIZE * len(dfs[i])) + train_end_idx\n",
    "    \n",
    "    train_dataset = StockDataset(\n",
    "        dfs[i],\n",
    "        label_name=LABEL_NAMES,\n",
    "        start_idx=0,\n",
    "        input_size=INPUT_SIZE,\n",
    "        end_idx=train_end_idx,\n",
    "        label_size=TIMESTAMP_PER_DAY, # predict to next day\n",
    "    )\n",
    "\n",
    "    val_dataset = StockDataset(\n",
    "        dfs[i],\n",
    "        label_name=LABEL_NAMES,\n",
    "        start_idx=train_end_idx,\n",
    "        input_size=INPUT_SIZE,\n",
    "        end_idx=val_end_idx,\n",
    "        label_size=TIMESTAMP_PER_DAY\n",
    "    )\n",
    "\n",
    "    test_dataset = StockDataset(\n",
    "        dfs[i],\n",
    "        label_name=LABEL_NAMES,\n",
    "        start_idx=val_end_idx,\n",
    "        input_size=INPUT_SIZE,\n",
    "        end_idx=len(dfs[i]),\n",
    "        label_size=TIMESTAMP_PER_DAY\n",
    "    )\n",
    "\n",
    "    train_dataset_list.append(train_dataset)\n",
    "    val_dataset_list.append(val_dataset)\n",
    "    test_dataset_list.append(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_size: int,\n",
    "                 num_layers: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim,\n",
    "                           hidden_size,\n",
    "                           num_layers,\n",
    "                           batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, (hidden_state, cell_state) = self.lstm(x)\n",
    "\n",
    "        return hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_dim: int,\n",
    "                 hidden_size: int,\n",
    "                 num_layers: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(output_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        outputs, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        preds = self.fc(outputs)\n",
    "\n",
    "        return preds, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: nn.Module,\n",
    "                 decoder: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self,\n",
    "                source: Tensor,\n",
    "                target: Tensor, \n",
    "                teaching_force_ratio: float = 0.5):\n",
    "        batch_size, tg_len, tg_dim = target.shape\n",
    "\n",
    "        # we have to push `outputs` tensor to device manually because of not being model parameters\n",
    "        outputs = torch.zeros(batch_size, tg_len, tg_dim).to(source.device)\n",
    "\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # get the first true label (unsqueeze to make the shape right)\n",
    "        input = source[:, 0, :].unsqueeze(1)\n",
    "\n",
    "        for t in range(1, tg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "\n",
    "            teaching_force = np.random.rand() > teaching_force_ratio\n",
    "            input = target[:, t, :] if teaching_force else output\n",
    "            if input.dim() < 3:\n",
    "                input = input.unsqueeze(1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(LABEL_NAMES)\n",
    "HIDDEN_SIZE = 50\n",
    "OUTPUT_DIM = INPUT_DIM\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, HIDDEN_SIZE, NUM_LAYERS)\n",
    "decoder = Decoder(OUTPUT_DIM, HIDDEN_SIZE, NUM_LAYERS)\n",
    "model = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "model.to(device)\n",
    "for i, (train_set, val_set) in enumerate(zip(train_dataset_list, val_dataset_list)):\n",
    "    print(f'Stock {i + 1}')\n",
    "    train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # train_running_loss = {'epoch': [],\n",
    "    #                       'loss': []}\n",
    "    # train_rmse = {'epoch': [],\n",
    "    #               'rmse': []}\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print(f'Epoch {e + 1}')\n",
    "        running_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        train_loop = tqdm(train_dataloader, desc=f'{\"Train\":^11}', leave=True)        \n",
    "        for b, data in enumerate(train_loop):\n",
    "            X, y = (_.to(device) for _ in data)\n",
    "\n",
    "            y_pred = model(X, y)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            logging_dict = {\n",
    "                'Loss': running_loss / (b + 1)\n",
    "            }\n",
    "            train_loop.set_postfix(logging_dict)\n",
    "\n",
    "        # model.eval()\n",
    "        # eval_loop = tqdm(val_dataloader, desc=f'{\"Eval\":^7}', leave=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
